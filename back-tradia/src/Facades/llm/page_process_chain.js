const workersLLM = require("./config/build_config.js");
const { z } = require("zod");
const LLM_WRAPPER = require("./utils/llm");
const html2img = require("../helpers/Html2Img");
const Mustache = require("mustache");
const JoinerHtml = require("../services/documents/joinHtmlDocuments");
class PAGE_PROCESS_CHAIN {
    static schema = z.object({
        process: z.object({
            process_model: z.any(),
            process_dir: z.string(),
            params: z.object({
                auto_correction_cycles: z.number().default(0),
                user_prompt: z.string().optional(),
                language: z.string(),
            }),
        }),
        page: z.object({
            image_path: z.string(),
            dimensions: z.object({
                width: z.number(),
                height: z.number(),
            }),
            page_number: z.number(),
        }),
        llm: z.instanceof(LLM_WRAPPER),
    });
    constructor(input) {
        this.input = PAGE_PROCESS_CHAIN.schema.parse(input);
        this.process = this.input.process;
        this.page = this.input.page;
        this.llm = this.input.llm;
        this.cycle_index = 0;
        this.workers = workersLLM;
        this.prev_workersLLM = [];
        this.current_workerLLM = this.workers[0];
        this.chain_is_done = false;
        this.history = [];
        this.responses = [];
        this.answer = null;
    }
    async run() {
        return {
            html: await this._execute_cycles(),
            page: this.page,
        };
    }
    async _execute_cycles() {
        while (this.cycle_index <= this.process.params.auto_correction_cycles) {
            if (this.chain_is_done) break;
            try {
                await this._execute_chain();
            } catch (error) {
                console.error("Error en _execute_chain:", error.message);
            }
            this.cycle_index++;
        }
        if (!this.answer)
            throw new Error("No answer was generated by the chain LLM");
        return this.answer;
    }
    async _execute_chain() {
        for (let index = 0; index < this.workers.length; index++) {
            if (this.chain_is_done) break;
            const work = this.workers[index];
            this.current_workerLLM = work;
            console.log(`Run ${work.name} in cycle ${this.cycle_index}`);
            const last_response = this.responses.at(-1);
            if (work.on_init)
                await work.on_init(last_response, this._build_hook_params());
            const work_prompt = work.get_prompt()
            console.log(work_prompt)
            const response = await this._run_llm({
                schema: work.schema,
                prompt: work_prompt,
                history: this.history,
            });
            this.responses.push(response);
            if (work.on_finish)
                await work.on_finish(response, this._build_hook_params());
        }
        this._send_status(`page: ${this.page.page_number} processing chain done (cycles ${this.cycle_index + 1}/${this.process.params.auto_correction_cycles + 1})`)
    }
    async _run_llm({ schema, prompt, history }) {
        const messages = this.llm.add_system_prompt({
            system_prompt: this._build_prompt_template(prompt),
            messages: history,
        });
        const response = await this.llm.generate_object({ schema, messages });
        return response;
    }
    _build_prompt_template(prompt) {
        const params_schema = z.object({
            page_index: z.number(),
            dimensions: z.object({
                width: z.number(),
                height: z.number(),
            }),
            output_language: z.string(),
            user_prompt: z.string(),
        });
        const params = params_schema.parse({
            page_index: this.page.page_number,
            dimensions: this.page.dimensions,
            output_language: this.process.params.language,
            user_prompt: this.process.params.user_prompt,
        });
        const final_system_prompt = Mustache.render(prompt, params);
        return final_system_prompt
    }
    _build_hook_params() {
        const add_message = this.llm.add_message.bind();
        function add_user_message(messages, { image, text }) {
            return add_message({ role: "user", messages, image, text });
        }
        function add_assistant_message(messages, { image, text }) {
            return add_message({ role: "assistant", messages, image, text });
        }
        return {
            chain: this,
            history: this.history,
            prev_workersLLM: this.prev_workersLLM,
            current_workerLLM: this.current_workerLLM,
            process_config: this.process,
            page: this.page,
            utils: {
                html2img: html2img.convertToImage,
                add_assistant_message,
                add_user_message,
                joiner_html: new JoinerHtml(),
            },
            get_answer_of_chainLLM: () => {
                return this._get_answer();
            },
            set_answer_of_chainLLM: (answer) => {
                this._set_answer(answer);
            },
            stop_chain: () => {
                this._stop_chain();
            },
            set_history: (messages) => {
                this._set_history(messages);
            },
        };
    }
    _get_answer() {
        return this.answer;
    }
    _set_answer(answer) {
        console.log("set answer!");
        this.answer = answer;
    }
    _set_history(messages) {
        this.history = messages;
    }
    _stop_chain() {
        console.log("stop chain!");
        this.chain_is_done = true;
    }
    _send_status(message) {
        try {
            this.process.process_model.update({ message })
        } catch (error) { }
    }
}
module.exports = PAGE_PROCESS_CHAIN;
